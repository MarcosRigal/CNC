{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title-cell"
      },
      "source": [
        "# Práctica 3: Clasificación Jerárquica \n",
        "## Opción 2: Comparación de métodos\n",
        "\n",
        "### Objetivo\n",
        "El objetivo de esta práctica es introducir y aplicar los conceptos de **clasificación jerárquica**, utilizando al menos **dos algoritmos** ya existentes en las bibliotecas indicadas, y **al menos tres problemas de clasificación jerárquica**. Posteriormente, compararemos sus resultados y expondremos las conclusiones más relevantes.\n",
        "\n",
        "En este notebook se realiza:\n",
        "1. La **carga y preparación** de varios datasets jerárquicos (obtenidos de repositorios recomendados).\n",
        "2. La **aplicación de dos métodos** de clasificación jerárquica (SVM y Random Forest) por medio de la biblioteca `sklearn-hierarchical-classification`.\n",
        "3. La **evaluación y comparación** de los resultados.\n",
        "4. La elaboración de **conclusiones** sobre el rendimiento y las características de cada método."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-libraries"
      },
      "source": [
        "## 1. Instalación de bibliotecas necesarias\n",
        "Se requiere instalar la biblioteca `sklearn-hierarchical-classification` (u otras equivalentes), así como clonar el repositorio que contiene los datasets utilizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-code"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/globality-corp/sklearn-hierarchical-classification.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone-repo"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/geantrindade/TEsHierarchicalDatasets.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load-libraries"
      },
      "source": [
        "## 2. Importación de librerías y definición de funciones\n",
        "A continuación, importamos las librerías y definimos las funciones auxiliares:\n",
        "- **Preprocesamiento de datos** (imputación y normalización)\n",
        "- **Creación de la jerarquía de clases** a partir de las etiquetas\n",
        "- **Cálculo de métricas jerárquicas** (precisión, recall, F1 jerárquicos)\n",
        "- **Entrenamiento y evaluación** de los clasificadores jerárquicos\n",
        "- **Visualización** de la jerarquía y comparación de resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "helper-code"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn_hierarchical_classification.classifier import HierarchicalClassifier\n",
        "from sklearn_hierarchical_classification.constants import ROOT\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import networkx as nx\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Función para preprocesar el dataset\n",
        "def preprocess_dataset(X, y, test_size=0.25, random_state=42):\n",
        "    \"\"\"Preprocesamiento: split, imputación y estandarización.\"\"\"\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "\n",
        "    # Imputar valores perdidos\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    X_train = imputer.fit_transform(X_train)\n",
        "    X_test = imputer.transform(X_test)\n",
        "\n",
        "    # Estandarizar\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Selección de características (top k basadas en ANOVA F-value)\n",
        "    selector = SelectKBest(f_classif, k=min(100, X_train.shape[1]))\n",
        "    X_train = selector.fit_transform(X_train, y_train)\n",
        "    X_test = selector.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Función para construir la jerarquía de clases\n",
        "def build_class_hierarchy(labels):\n",
        "    G = nx.DiGraph()\n",
        "    G.add_node(ROOT)\n",
        "\n",
        "    unique_labels = sorted(set(labels))\n",
        "    for label in unique_labels:\n",
        "        parts = label.split('.')\n",
        "        G.add_node(label)\n",
        "        if len(parts) > 1:\n",
        "            parent = '.'.join(parts[:-1])\n",
        "            if parent in unique_labels:\n",
        "                G.add_edge(parent, label)\n",
        "            else:\n",
        "                G.add_edge(ROOT, label)\n",
        "        else:\n",
        "            G.add_edge(ROOT, label)\n",
        "\n",
        "    return G\n",
        "\n",
        "# Métricas jerárquicas personalizadas\n",
        "def custom_hierarchical_fbeta(y_true, y_pred, class_hierarchy, beta=1.0):\n",
        "    precision = custom_hierarchical_precision(y_true, y_pred, class_hierarchy)\n",
        "    recall = custom_hierarchical_recall(y_true, y_pred, class_hierarchy)\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
        "\n",
        "def custom_hierarchical_precision(y_true, y_pred, class_hierarchy):\n",
        "    total_correct = 0\n",
        "    total_predicted = 0\n",
        "\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        pred_ancestors = set(nx.ancestors(class_hierarchy, pred))\n",
        "        pred_ancestors.add(pred)\n",
        "        true_ancestors = set(nx.ancestors(class_hierarchy, true))\n",
        "        true_ancestors.add(true)\n",
        "\n",
        "        if ROOT in pred_ancestors:\n",
        "            pred_ancestors.remove(ROOT)\n",
        "        if ROOT in true_ancestors:\n",
        "            true_ancestors.remove(ROOT)\n",
        "\n",
        "        correct = len(pred_ancestors.intersection(true_ancestors))\n",
        "        total_correct += correct\n",
        "        total_predicted += len(pred_ancestors)\n",
        "\n",
        "    if total_predicted == 0:\n",
        "        return 0.0\n",
        "    return total_correct / total_predicted\n",
        "\n",
        "def custom_hierarchical_recall(y_true, y_pred, class_hierarchy):\n",
        "    total_correct = 0\n",
        "    total_actual = 0\n",
        "\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        pred_ancestors = set(nx.ancestors(class_hierarchy, pred))\n",
        "        pred_ancestors.add(pred)\n",
        "        true_ancestors = set(nx.ancestors(class_hierarchy, true))\n",
        "        true_ancestors.add(true)\n",
        "\n",
        "        if ROOT in pred_ancestors:\n",
        "            pred_ancestors.remove(ROOT)\n",
        "        if ROOT in true_ancestors:\n",
        "            true_ancestors.remove(ROOT)\n",
        "\n",
        "        correct = len(pred_ancestors.intersection(true_ancestors))\n",
        "        total_correct += correct\n",
        "        total_actual += len(true_ancestors)\n",
        "\n",
        "    if total_actual == 0:\n",
        "        return 0.0\n",
        "    return total_correct / total_actual\n",
        "\n",
        "# Función de evaluación: calcula accuracy y métricas jerárquicas\n",
        "def evaluate_classifier(clf, X_test, y_test, classifier_name, dataset_name):\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    h_precision = custom_hierarchical_precision(y_test, y_pred, clf.graph_)\n",
        "    h_recall = custom_hierarchical_recall(y_test, y_pred, clf.graph_)\n",
        "    h_f1 = custom_hierarchical_fbeta(y_test, y_pred, clf.graph_, beta=1.0)\n",
        "\n",
        "    print(f\"{classifier_name} en {dataset_name}:\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  F1 Jerárquico: {h_f1:.4f}\")\n",
        "    print(f\"  Precisión Jerárquica: {h_precision:.4f}\")\n",
        "    print(f\"  Recall Jerárquico: {h_recall:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'h_f1': h_f1,\n",
        "        'h_precision': h_precision,\n",
        "        'h_recall': h_recall\n",
        "    }\n",
        "\n",
        "# Función para visualizar la jerarquía\n",
        "def visualize_hierarchy(graph, title=\"Jerarquía de Clases\"):\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    pos = nx.spring_layout(graph)\n",
        "    nx.draw(graph, pos, with_labels=True, node_color='lightblue',\n",
        "            node_size=1500, arrowsize=15, font_size=8)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Funciones de entrenamiento con los dos clasificadores base\n",
        "def train_evaluate_hsvm(X_train, X_test, y_train, y_test, class_hierarchy, dataset_name):\n",
        "    clf = HierarchicalClassifier(\n",
        "        base_estimator=SVC(kernel='linear', probability=True, C=1.0),\n",
        "        class_hierarchy=class_hierarchy,\n",
        "        algorithm='lcpn',\n",
        "        prediction_depth='mlnp',\n",
        "    )\n",
        "    print(f\"Entrenando Hierarchical SVM en {dataset_name}...\")\n",
        "    clf.fit(X_train, y_train)\n",
        "    return evaluate_classifier(clf, X_test, y_test, \"Hierarchical SVM\", dataset_name)\n",
        "\n",
        "def train_evaluate_hrf(X_train, X_test, y_train, y_test, class_hierarchy, dataset_name):\n",
        "    clf = HierarchicalClassifier(\n",
        "        base_estimator=RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        class_hierarchy=class_hierarchy,\n",
        "        algorithm='lcpn',\n",
        "        prediction_depth='mlnp',\n",
        "    )\n",
        "    print(f\"Entrenando Hierarchical Random Forest en {dataset_name}...\")\n",
        "    clf.fit(X_train, y_train)\n",
        "    return evaluate_classifier(clf, X_test, y_test, \"Hierarchical Random Forest\", dataset_name)\n",
        "\n",
        "# Función para cargar los datasets\n",
        "def load_datasets():\n",
        "    print(\"Cargando datasets...\")\n",
        "    datasets = {}\n",
        "\n",
        "    # MIPS Kmers\n",
        "    print(\"- MIPS Kmers\")\n",
        "    df_mips_kmers = pd.read_csv('/content/TEsHierarchicalDatasets/csv/mips_kmers.csv')\n",
        "    y_column = 'classe' if 'classe' in df_mips_kmers.columns else 'Class'\n",
        "    X_mips_kmers = df_mips_kmers.drop([y_column], axis=1, errors='ignore').values\n",
        "    y_mips_kmers = df_mips_kmers[y_column].values\n",
        "    datasets[\"MIPS Kmers\"] = (X_mips_kmers, y_mips_kmers)\n",
        "\n",
        "    # MIPS Pseudo\n",
        "    print(\"- MIPS Pseudo\")\n",
        "    df_mips_pseudo = pd.read_csv('/content/TEsHierarchicalDatasets/csv/mips_pseudo.csv')\n",
        "    y_column = 'classe' if 'classe' in df_mips_pseudo.columns else 'Class'\n",
        "    X_mips_pseudo = df_mips_pseudo.drop([y_column], axis=1, errors='ignore').values\n",
        "    y_mips_pseudo = df_mips_pseudo[y_column].values\n",
        "    datasets[\"MIPS Pseudo\"] = (X_mips_pseudo, y_mips_pseudo)\n",
        "\n",
        "    # Repbase Kmers\n",
        "    print(\"- Repbase Kmers\")\n",
        "    df_repbase_kmers = pd.read_csv('/content/TEsHierarchicalDatasets/csv/repbase_kmers.csv')\n",
        "    y_column = 'classe' if 'classe' in df_repbase_kmers.columns else 'Class'\n",
        "    X_repbase_kmers = df_repbase_kmers.drop([y_column], axis=1, errors='ignore').values\n",
        "    y_repbase_kmers = df_repbase_kmers[y_column].values\n",
        "    datasets[\"Repbase Kmers\"] = (X_repbase_kmers, y_repbase_kmers)\n",
        "\n",
        "    return datasets\n",
        "\n",
        "# Función para graficar comparaciones\n",
        "def plot_metric_comparison(results, metric='accuracy', title=None):\n",
        "    datasets = list(results.keys())\n",
        "    hsvm_values = [results[dataset][\"HSVM\"][metric] for dataset in datasets]\n",
        "    hrf_values = [results[dataset][\"HRF\"][metric] for dataset in datasets]\n",
        "    x = np.arange(len(datasets))\n",
        "    width = 0.35\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    ax.bar(x - width/2, hsvm_values, width, label='Hierarchical SVM')\n",
        "    ax.bar(x + width/2, hrf_values, width, label='Hierarchical Random Forest')\n",
        "\n",
        "    metric_name = metric.replace('h_', 'Jerárquico ').title()\n",
        "    ax.set_ylabel(metric_name)\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "    else:\n",
        "        ax.set_title(f'{metric_name} por Algoritmo y Dataset')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(datasets, rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "    plt.ylim(0, 1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run-experiments"
      },
      "source": [
        "## 3. Carga de datos, entrenamiento y evaluación\n",
        "En esta sección:\n",
        "1. **Cargamos** los datasets jerárquicos.\n",
        "2. **Preprocesamos** cada dataset (split, imputación, escalado, selección de características).\n",
        "3. **Construimos** su jerarquía de clases.\n",
        "4. **Entrenamos** ambos algoritmos (SVM y RandomForest) usando la librería `sklearn-hierarchical-classification`.\n",
        "5. **Evaluamos** cada modelo obteniendo *accuracy*, *F1 jerárquico*, *precisión jerárquica* y *recall jerárquico*.\n",
        "6. **Visualizamos** las jerarquías y almacenamos los resultados para compararlos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "execute-code"
      },
      "outputs": [],
      "source": [
        "print(\"Iniciando experimentos de clasificación jerárquica...\")\n",
        "all_datasets = load_datasets()\n",
        "results = {}\n",
        "\n",
        "for dataset_name, (X, y) in all_datasets.items():\n",
        "    print(f\"\\nProcesando dataset: {dataset_name}\")\n",
        "    X_train, X_test, y_train, y_test = preprocess_dataset(X, y)\n",
        "    class_hierarchy = build_class_hierarchy(y_train)\n",
        "\n",
        "    # Visualización de la jerarquía\n",
        "    visualize_hierarchy(class_hierarchy, f\"{dataset_name} - Jerarquía de Clases\")\n",
        "\n",
        "    # Entrenamiento y evaluación SVM\n",
        "    hsvm_results = train_evaluate_hsvm(X_train, X_test, y_train, y_test, class_hierarchy, dataset_name)\n",
        "\n",
        "    # Entrenamiento y evaluación Random Forest\n",
        "    hrf_results = train_evaluate_hrf(X_train, X_test, y_train, y_test, class_hierarchy, dataset_name)\n",
        "\n",
        "    results[dataset_name] = {\n",
        "        \"HSVM\": hsvm_results,\n",
        "        \"HRF\": hrf_results\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plot-results"
      },
      "source": [
        "## 4. Comparación de resultados\n",
        "Para cada uno de los **datasets** probados, se comparan las métricas de *Accuracy*, *F1 Jerárquico*, *Precisión Jerárquica* y *Recall Jerárquico* de los dos algoritmos.\n",
        "\n",
        "En las gráficas de barras se pueden observar las diferencias entre los métodos:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot-metrics-code"
      },
      "outputs": [],
      "source": [
        "print(\"\\nGenerando gráficas de comparación...\")\n",
        "plot_metric_comparison(results, 'accuracy', 'Comparación de Accuracy')\n",
        "plot_metric_comparison(results, 'h_f1', 'Comparación de F1 Jerárquico')\n",
        "plot_metric_comparison(results, 'h_precision', 'Comparación de Precisión Jerárquica')\n",
        "plot_metric_comparison(results, 'h_recall', 'Comparación de Recall Jerárquico')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary-findings"
      },
      "source": [
        "## 5. Resumen de resultados y conclusiones\n",
        "A continuación, se muestra un resumen numérico de los resultados y se discuten las principales conclusiones:\n",
        "1. **Comparación de Algoritmos**: Se observa qué algoritmo obtiene mejores métricas en promedio.\n",
        "2. **Dificultad de los Datasets**: Cuáles son más complicados, según las métricas globales.\n",
        "3. **Métricas Jerárquicas**: Qué tan bien capturan la estructura jerárquica en comparación con la métrica de *accuracy* convencional.\n",
        "4. **Conclusiones Globales**: Recomendaciones finales acerca de cuál método se comporta mejor y en qué casos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "summary-code"
      },
      "outputs": [],
      "source": [
        "print(\"\\n=== RESUMEN DE RESULTADOS ===\")\n",
        "# 1. Comparación de Algoritmos\n",
        "datasets_list = list(results.keys())\n",
        "avg_hsvm_acc = np.mean([results[d][\"HSVM\"][\"accuracy\"] for d in results])\n",
        "avg_hsvm_f1 = np.mean([results[d][\"HSVM\"][\"h_f1\"] for d in results])\n",
        "avg_hrf_acc = np.mean([results[d][\"HRF\"][\"accuracy\"] for d in results])\n",
        "avg_hrf_f1 = np.mean([results[d][\"HRF\"][\"h_f1\"] for d in results])\n",
        "\n",
        "print(\"1. Rendimiento por Algoritmo y Dataset:\")\n",
        "for d in datasets_list:\n",
        "    hsvm_acc = results[d][\"HSVM\"][\"accuracy\"]\n",
        "    hsvm_f1 = results[d][\"HSVM\"][\"h_f1\"]\n",
        "    hrf_acc = results[d][\"HRF\"][\"accuracy\"]\n",
        "    hrf_f1 = results[d][\"HRF\"][\"h_f1\"]\n",
        "    print(f\" - {d}\")\n",
        "    print(f\"   HSVM: Accuracy={hsvm_acc:.4f}, F1j={hsvm_f1:.4f}\")\n",
        "    print(f\"   HRF:  Accuracy={hrf_acc:.4f}, F1j={hrf_f1:.4f}\")\n",
        "\n",
        "print(\"\\n2. Promedios Globales:\")\n",
        "print(f\" - HSVM: Accuracy promedio={avg_hsvm_acc:.4f}, F1j promedio={avg_hsvm_f1:.4f}\")\n",
        "print(f\" - HRF:  Accuracy promedio={avg_hrf_acc:.4f}, F1j promedio={avg_hrf_f1:.4f}\")\n",
        "\n",
        "# 2. Análisis de dificultad de los Datasets\n",
        "hsvm_accs = [results[d][\"HSVM\"][\"accuracy\"] for d in datasets_list]\n",
        "hrf_accs = [results[d][\"HRF\"][\"accuracy\"] for d in datasets_list]\n",
        "avg_accs = [(a + b)/2 for a, b in zip(hsvm_accs, hrf_accs)]\n",
        "dataset_difficulty = list(zip(datasets_list, avg_accs))\n",
        "dataset_difficulty.sort(key=lambda x: x[1])\n",
        "\n",
        "print(\"\\n3. Datasets ordenados por dificultad (mayor dificultad = menor accuracy promedio):\")\n",
        "for ds, acc in dataset_difficulty:\n",
        "    print(f\" - {ds}: Accuracy promedio = {acc:.4f}\")\n",
        "\n",
        "# 3. Conclusiones sobre métricas jerárquicas\n",
        "print(\"\\n4. Métricas Jerárquicas:\")\n",
        "for d in datasets_list:\n",
        "    hsvm_h_prec = results[d][\"HSVM\"][\"h_precision\"]\n",
        "    hsvm_h_rec = results[d][\"HSVM\"][\"h_recall\"]\n",
        "    hrf_h_prec = results[d][\"HRF\"][\"h_precision\"]\n",
        "    hrf_h_rec = results[d][\"HRF\"][\"h_recall\"]\n",
        "    print(f\" - {d}:\")\n",
        "    print(f\"   HSVM: Prec. jerárq={hsvm_h_prec:.4f}, Recall jerárq={hsvm_h_rec:.4f}\")\n",
        "    print(f\"   HRF:  Prec. jerárq={hrf_h_prec:.4f}, Recall jerárq={hrf_h_rec:.4f}\")\n",
        "\n",
        "print(\"\\n5. Conclusiones globales:\")\n",
        "# Comparación general de accuracy\n",
        "if avg_hsvm_acc > avg_hrf_acc:\n",
        "    print(\" - El método HSVM obtiene mejor accuracy en promedio.\")\n",
        "elif avg_hrf_acc > avg_hsvm_acc:\n",
        "    print(\" - El método HRF obtiene mejor accuracy en promedio.\")\n",
        "else:\n",
        "    print(\" - Ambos métodos tienen resultados muy similares en accuracy promedio.\")\n",
        "\n",
        "# Comparación general de F1 jerárquico\n",
        "if avg_hsvm_f1 > avg_hrf_f1:\n",
        "    print(\" - HSVM parece capturar mejor la estructura jerárquica, gracias a un F1 jerárquico promedio más alto.\")\n",
        "elif avg_hrf_f1 > avg_hsvm_f1:\n",
        "    print(\" - HRF parece capturar mejor la estructura jerárquica, gracias a un F1 jerárquico promedio más alto.\")\n",
        "else:\n",
        "    print(\" - Ambos métodos tienen F1 jerárquico muy similar en promedio.\")\n",
        "\n",
        "print(\"\\n - En general, la elección de un método podría depender de las características del dataset.\")\n",
        "print(\" - Se recomienda probar ambos clasificadores cuando se aborde un nuevo problema de clasificación jerárquica.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
