{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Práctica 4: Clasificación Semi-Supervisada\n",
        "\n",
        "**Objetivo**: El objetivo de esta práctica es introducir los conceptos de clasificación semi-supervisada.\n",
        "\n",
        "La práctica consiste en dos tareas:\n",
        "\n",
        "## TAREA 1: Implementación de un método\n",
        "- Seleccione un algoritmo de los indicados en la teoría que implemente aprendizaje semi-supervisado de cualquiera de los paradigmas estudiados.\n",
        "- Implemente el algoritmo.\n",
        "- Seleccione al menos un dataset semi-supervisado y evalúe el algoritmo implementado.\n",
        "\n",
        "## TAREA 2: Comparación de métodos\n",
        "1. Seleccione al menos dos algoritmos de los disponibles en las bibliotecas indicadas.\n",
        "2. Seleccione al menos tres problemas semi-supervisados de los repositorios indicados.\n",
        "3. Aplique los algoritmos seleccionados a los datasets.\n",
        "4. Compare los resultados y explique las conclusiones obtenidas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "562f8ad6",
      "metadata": {},
      "source": [
        "# TAREA 1: Implementación de un método de Self-Training\n",
        "En esta sección se muestra la implementación propia de un algoritmo de **Self-Training** y su posterior evaluación en un dataset sintético."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84361f87",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.base import clone\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def self_training(X_labeled, y_labeled, X_unlabeled, base_classifier,\n",
        "                  confidence_threshold=0.9, max_iter=10):\n",
        "    \"\"\"\n",
        "    Self-Training method.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_labeled : array-like, shape (n_labeled_samples, n_features)\n",
        "        Labeled dataset.\n",
        "    y_labeled : array-like, shape (n_labeled_samples,)\n",
        "        Corresponding labels for X_labeled.\n",
        "    X_unlabeled : array-like, shape (n_unlabeled_samples, n_features)\n",
        "        Unlabeled dataset.\n",
        "    base_classifier : scikit-learn estimator\n",
        "        Base classifier that must support at least predict_proba.\n",
        "    confidence_threshold : float, optional (default=0.9)\n",
        "        Minimum probability threshold to accept pseudo-labeled instances.\n",
        "    max_iter : int, optional (default=10)\n",
        "        Maximum number of self-training iterations.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    trained_classifier : scikit-learn estimator\n",
        "        The trained classifier after self-training.\n",
        "    \"\"\"\n",
        "    X_l = X_labeled.copy()\n",
        "    y_l = y_labeled.copy()\n",
        "    X_u = X_unlabeled.copy()\n",
        "\n",
        "    for i in range(1, max_iter + 1):\n",
        "        current_clf = clone(base_classifier)\n",
        "        current_clf.fit(X_l, y_l)\n",
        "\n",
        "        if hasattr(current_clf, \"predict_proba\"):\n",
        "            probs = current_clf.predict_proba(X_u)\n",
        "            pred_labels = np.argmax(probs, axis=1)\n",
        "            max_probs = np.max(probs, axis=1)\n",
        "        else:\n",
        "            pred_labels = current_clf.predict(X_u)\n",
        "            max_probs = np.ones(len(X_u))\n",
        "\n",
        "        high_conf_idx = np.where(max_probs >= confidence_threshold)[0]\n",
        "        if len(high_conf_idx) == 0:\n",
        "            print(f\"Iteración {i}: No se encontraron instancias con confianza >= {confidence_threshold}. Finalizando.\")\n",
        "            break\n",
        "\n",
        "        X_l = np.vstack([X_l, X_u[high_conf_idx]])\n",
        "        y_l = np.hstack([y_l, pred_labels[high_conf_idx]])\n",
        "        X_u = np.delete(X_u, high_conf_idx, axis=0)\n",
        "\n",
        "        print(f\"Iteración {i}: Se añadieron {len(high_conf_idx)} instancias con alta confianza.\")\n",
        "\n",
        "        if len(X_u) == 0:\n",
        "            print(f\"Iteración {i}: No quedan instancias no etiquetadas. Finalizando.\")\n",
        "            break\n",
        "\n",
        "    final_clf = clone(base_classifier)\n",
        "    final_clf.fit(X_l, y_l)\n",
        "\n",
        "    return final_clf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cab32e1c",
      "metadata": {},
      "source": [
        "### Ejemplo de uso en un dataset real (Digits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53f4d1ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Carga de un dataset real (Digits)\n",
        "data = load_digits()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# División de datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Seleccionamos un porcentaje de datos etiquetados\n",
        "labeled_percentage = 0.2\n",
        "num_labeled = int(len(y_train) * labeled_percentage)\n",
        "\n",
        "X_labeled = X_train[:num_labeled]\n",
        "y_labeled = y_train[:num_labeled]\n",
        "X_unlabeled = X_train[num_labeled:]\n",
        "\n",
        "# Entrenamiento con RandomForestClassifier utilizando Self-Training\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "self_trained_model = self_training(\n",
        "    X_labeled,\n",
        "    y_labeled,\n",
        "    X_unlabeled,\n",
        "    rf,\n",
        "    confidence_threshold=0.9,\n",
        "    max_iter=10\n",
        ")\n",
        "\n",
        "# Evaluación en el conjunto de prueba\n",
        "y_pred = self_trained_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Exactitud en test tras Self-Training:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c20e7d99",
      "metadata": {},
      "source": [
        "# TAREA 2: Comparación de métodos\n",
        "\n",
        "En esta segunda parte, se comparan dos métodos de clasificación semi-supervisada:\n",
        "- **Label Spreading**  \n",
        "- **Self-Training**\n",
        "\n",
        "Se emplean **tres datasets** de scikit-learn: *Iris*, *Wine* y *Breast Cancer*. Se enmascara un porcentaje de las etiquetas (por ejemplo, el 50%) para simular un escenario semi-supervisado y luego se entrena y evalúa cada modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "370ffa38",
      "metadata": {},
      "source": [
        "### Importaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c336c544",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.semi_supervised import LabelSpreading, SelfTrainingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.datasets import load_iris, load_wine, load_breast_cancer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df947ce7",
      "metadata": {},
      "source": [
        "### Carga de datasets con etiquetas ocultas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc2c4c7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_real_datasets(mask_ratio=0.5, random_state=42):\n",
        "    \"\"\"\n",
        "    Carga tres datasets de sklearn (Iris, Wine, Breast Cancer) y oculta una fracción (mask_ratio)\n",
        "    de sus etiquetas para simular un escenario semi-supervisado.\n",
        "    Retorna un diccionario con {nombre_dataset: (X, y_masked)}.\n",
        "    \"\"\"\n",
        "    datasets = {}\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    for name, loader in zip([\"Iris\", \"Wine\", \"Breast Cancer\"],\n",
        "                            [load_iris, load_wine, load_breast_cancer]):\n",
        "        data = loader()\n",
        "        X, y = data.data, data.target\n",
        "        y_masked = y.copy()\n",
        "\n",
        "        # Índices aleatorios a los que se les quita la etiqueta\n",
        "        mask = rng.choice(len(y_masked), size=int(mask_ratio * len(y_masked)), replace=False)\n",
        "        y_masked[mask] = -1  # Se coloca -1 donde no hay etiqueta\n",
        "\n",
        "        # Asegurar que todas las clases mantengan al menos una etiqueta\n",
        "        while len(set(y_masked[y_masked != -1])) < len(set(y)):\n",
        "            mask = rng.choice(len(y_masked), size=int(mask_ratio * len(y_masked)), replace=False)\n",
        "            y_masked = y.copy()\n",
        "            y_masked[mask] = -1\n",
        "\n",
        "        datasets[name] = (X, y_masked)\n",
        "\n",
        "    return datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2a5ccc2",
      "metadata": {},
      "source": [
        "### Función para evaluar un modelo semi-supervisado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d381167",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_semi_supervised(model, X, y, dataset_name):\n",
        "    \"\"\"\n",
        "    Divide X, y en train y test; entrena 'model' y evalúa las métricas\n",
        "    únicamente en las instancias del test que sí tengan etiqueta real.\n",
        "    \"\"\"\n",
        "    # Dividir en train/test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # Entrenar\n",
        "    try:\n",
        "        model.fit(X_train, y_train)\n",
        "    except ValueError as e:\n",
        "        print(f\"[ERROR] No se pudo entrenar {dataset_name} con {model.__class__.__name__}: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Filtrar muestras etiquetadas en test\n",
        "    test_labeled_mask = (y_test != -1)\n",
        "    X_test_labeled, y_test_labeled = X_test[test_labeled_mask], y_test[test_labeled_mask]\n",
        "\n",
        "    if len(y_test_labeled) == 0:\n",
        "        print(f\"[ADVERTENCIA] No hay muestras etiquetadas en el test para {dataset_name}.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Predicción\n",
        "    try:\n",
        "        y_pred = model.predict(X_test_labeled)\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] No se pudo predecir {dataset_name} con {model.__class__.__name__}: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Cálculo de métricas\n",
        "    acc = accuracy_score(y_test_labeled, y_pred)\n",
        "    f1  = f1_score(y_test_labeled, y_pred, average='macro', zero_division=1)\n",
        "    prec = precision_score(y_test_labeled, y_pred, average='macro', zero_division=1)\n",
        "    rec  = recall_score(y_test_labeled, y_pred, average='macro', zero_division=1)\n",
        "\n",
        "    # Mostrar resultados\n",
        "    print(f\"\\nResultados en {dataset_name} con {model.__class__.__name__}:\")\n",
        "    print(f\"Accuracy: {acc:.4f}, F1-score: {f1:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}\")\n",
        "    print(\"Matriz de Confusión:\")\n",
        "    print(confusion_matrix(y_test_labeled, y_pred))\n",
        "    print(\"\\nReporte de clasificación:\")\n",
        "    print(classification_report(y_test_labeled, y_pred, zero_division=1))\n",
        "\n",
        "    # Visualización de la matriz de confusión\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(confusion_matrix(y_test_labeled, y_pred), annot=True,\n",
        "                cmap=\"Blues\", fmt=\"d\")\n",
        "    plt.title(f\"Matriz de Confusión - {dataset_name}\")\n",
        "    plt.xlabel(\"Predicción\")\n",
        "    plt.ylabel(\"Valor Real\")\n",
        "    plt.show()\n",
        "\n",
        "    return acc, f1, prec, rec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb4046e5",
      "metadata": {},
      "source": [
        "### Entrenamiento y evaluación con Label Spreading y Self-Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09d2236a",
      "metadata": {},
      "outputs": [],
      "source": [
        "real_datasets = load_real_datasets(mask_ratio=0.5)\n",
        "\n",
        "# Definir los métodos a comparar\n",
        "models = {\n",
        "    'Label Spreading': LabelSpreading(kernel='knn', n_neighbors=5, max_iter=50),\n",
        "    'Self-Training':   SelfTrainingClassifier(SVC(probability=True, C=10, gamma=0.01), max_iter=50)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Evaluar cada dataset con cada modelo\n",
        "for dataset_name, (X, y) in real_datasets.items():\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\n=== {dataset_name} | {model_name} ===\")\n",
        "        metrics = evaluate_semi_supervised(model, X, y, dataset_name)\n",
        "        results[(dataset_name, model_name)] = metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09d55ac6",
      "metadata": {},
      "source": [
        "### Gráficas comparativas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55d638ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extraer los nombres de dataset\n",
        "dataset_names = list(real_datasets.keys())\n",
        "accuracies, f1_scores = [], []\n",
        "\n",
        "for ds in dataset_names:\n",
        "    acc_ls, f1_ls, _, _ = results[(ds, 'Label Spreading')]\n",
        "    acc_st, f1_st, _, _ = results[(ds, 'Self-Training')]\n",
        "    accuracies.append([acc_ls, acc_st])\n",
        "    f1_scores.append([f1_ls, f1_st])\n",
        "\n",
        "accuracies = np.array(accuracies)\n",
        "f1_scores = np.array(f1_scores)\n",
        "x = np.arange(len(dataset_names))\n",
        "width = 0.35\n",
        "\n",
        "# Comparación de Accuracy\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "ax.bar(x - width/2, accuracies[:, 0], width, label='Label Spreading')\n",
        "ax.bar(x + width/2, accuracies[:, 1], width, label='Self-Training')\n",
        "ax.set_ylabel(\"Accuracy\")\n",
        "ax.set_title(\"Comparación de Accuracy\")\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(dataset_names)\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Comparación de F1-score\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "ax.bar(x - width/2, f1_scores[:, 0], width, label='Label Spreading')\n",
        "ax.bar(x + width/2, f1_scores[:, 1], width, label='Self-Training')\n",
        "ax.set_ylabel(\"F1-score (macro)\")\n",
        "ax.set_title(\"Comparación de F1-score\")\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(dataset_names)\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discusión de Resultados\n",
        "\n",
        "A modo de resumen, se pueden extraer algunas conclusiones tras los experimentos:\n",
        "\n",
        "1. **Rendimiento de Label Spreading vs. Self-Training**  \n",
        "   - En datasets donde la estructura de los datos favorece la propagación de etiquetas (por ejemplo, cuando las muestras de la misma clase están bien agrupadas), *Label Spreading* tiende a sobresalir.  \n",
        "   - *Self-Training* depende en gran medida de la capacidad del clasificador base y de la elección del umbral de confianza. Con un buen modelo inicial (por ejemplo, un SVC ajustado) y un umbral adecuado, puede ser muy competitivo.\n",
        "\n",
        "2. **Proporción de datos no etiquetados**  \n",
        "   - En este caso, se ha utilizado un 50% de muestras etiquetadas y otro 50% sin etiqueta. Si la fracción de etiquetas reales disminuye aún más, suele empeorar el rendimiento de ambos métodos, aunque la magnitud de la degradación depende de la naturaleza del dataset.\n",
        "\n",
        "3. **Variabilidad entre datasets**  \n",
        "   - Cada dataset presenta desafíos distintos (número de características, balance de clases, separabilidad). *Iris* suele ser más sencillo y ambos métodos logran alta precisión.  \n",
        "   - *Wine* y *Breast Cancer* pueden mostrar mayor complejidad, evidenciando diferencias en la capacidad de cada método para aprovechar los datos sin etiqueta.\n",
        "\n",
        "En general, ambos métodos de aprendizaje semi-supervisado son útiles en escenarios con escasez de etiquetas. No obstante, la selección final puede basarse en la geometría del problema, las clases, el clasificador base y el ajuste de hiperparámetros (por ejemplo, número de vecinos en *Label Spreading*, umbral de confianza en *Self-Training*, etc.)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
